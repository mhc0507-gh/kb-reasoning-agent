{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fee9a4",
   "metadata": {},
   "source": [
    "# Reasoning Diagnostic Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc41b31",
   "metadata": {},
   "source": [
    "### Check Python version\n",
    "\n",
    "Tested version: Python 3.12.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d0f44",
   "metadata": {},
   "source": [
    "### Load API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e4e8f",
   "metadata": {},
   "source": [
    "### Generate and install requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "chromadb==1.1.1\n",
    "fastmcp==2.12.4\n",
    "langchain==0.3.27\n",
    "langchain-core==0.3.83\n",
    "langchain-google-genai==2.1.12\n",
    "langchain-mcp-adapters==0.1.10\n",
    "langgraph==0.5.4\n",
    "mcp==1.16.0\n",
    "nest_asyncio\n",
    "sentence-transformers==5.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8fce5",
   "metadata": {},
   "source": [
    "### Define Knowledge Base documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef2adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = [\n",
    "\"\"\"\n",
    "Title: Connection Rejected Diagnostic\n",
    "Steps:\n",
    "    1. Check connection response code.\n",
    "    2. If error code, issue is indicated by the error code.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Title: High link latency diagnostic\n",
    "Steps:\n",
    "    1. Check link status.\n",
    "    2. If disconnected, issue is caused by bad link state.\n",
    "    3. If connected, check current system load.\n",
    "    4. Also check system load 5 minutes ago.\n",
    "    5. If both are above 90% then system is overloaded.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Title: Connection Failure Diagnostic\n",
    "Steps:\n",
    "    1. Check link status.\n",
    "    2. If disconnected, issue is caused by bad link state.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Title: Origin service {service-id} with high latency on more than 90% of requests in the last hour\n",
    "Steps:\n",
    "    1. Get percentage of requests with high latency for {service-id} in the last hour\n",
    "    2. If percentage of requests with high latency is less than 90% then report root cause as \"alert error\"\n",
    "    3. Get average end-to-end latency for {service-id} requests in the last hour\n",
    "    4. Get average server latency for {service-id} requests in the last hour\n",
    "    5a. If server latency is less than 10% of end-to-end latency then report root cause as \"high latency caused by external factors\"\n",
    "    5b. If storage latency is more than 50% of end-to-end latency then report root cause as \"high latency caused by storage\"\n",
    "    6. Get {deployment-id} where {service-id} is running\n",
    "    7. Get average CPU load of {deployment-id} in the last hour\n",
    "    8. If CPU load is above 90% then go to \"High CPU usage in {deployment-id} diagnostic\" KB article\n",
    "    9. Otherwise report root cause as \"unable to determine cause of high latency\"\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Title: High CPU usage in {deployment-id} diagnostic\n",
    "Steps:\n",
    "    1. Get average number of requests per second for {deployment-id} in the last hour\n",
    "    2. Get number of role instances for {deployment-id}\n",
    "    3. If number of requests per second per role instance is greater than 100 then report root cause as \"system overloaded with too many requests\"\n",
    "    4. Otherwise report root cause as \"unable to determine cause of high CPU usage\"\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faceb94",
   "metadata": {},
   "source": [
    "### Implement RAG for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class DiagnosticKB:\n",
    "    def __init__(self):\n",
    "        # RAG configuration\n",
    "        self.sample_documents = sample_documents\n",
    "\n",
    "        # Initialize RAG components\n",
    "        self.initialize_rag()\n",
    "\n",
    "\n",
    "    def initialize_rag(self):\n",
    "        \"\"\"Initialize RAG components including ChromaDB and sentence transformer\"\"\"\n",
    "        try:\n",
    "            # Initialize sentence transformer model\n",
    "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "            # Initialize ChromaDB client\n",
    "            self.chroma_client = chromadb.Client()\n",
    "\n",
    "            # Create or get collection\n",
    "            self.collection = self.chroma_client.get_or_create_collection(\n",
    "                name=\"knowledge_base\",\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "\n",
    "            # Add documents to collection\n",
    "            self.add_documents_to_collection()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing RAG: {str(e)}\")\n",
    "\n",
    "\n",
    "    def add_documents_to_collection(self):\n",
    "        \"\"\"Add sample documents to ChromaDB collection\"\"\"\n",
    "        try:\n",
    "            # Generate embeddings for documents\n",
    "            embeddings = self.embedding_model.encode(self.sample_documents)\n",
    "\n",
    "            # Prepare documents for ChromaDB\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            ids = []\n",
    "\n",
    "            for i, doc in enumerate(self.sample_documents):\n",
    "                documents.append(doc)\n",
    "                metadatas.append({\"source\": \"sample_document\", \"index\": i})\n",
    "                ids.append(f\"doc_{i}\")\n",
    "\n",
    "            # Add to collection\n",
    "            self.collection.add(\n",
    "                documents=documents,\n",
    "                embeddings=embeddings.tolist(),\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to collection: {str(e)}\")\n",
    "\n",
    "\n",
    "    def query_rag(self, query, n_results=1):\n",
    "        \"\"\"Query the RAG system to retrieve relevant documents\"\"\"\n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "            # Query the collection\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=query_embedding.tolist(),\n",
    "                n_results=n_results\n",
    "            )\n",
    "\n",
    "            # Extract relevant documents\n",
    "            relevant_docs = results['documents'][0] if results['documents'] else []\n",
    "\n",
    "            return relevant_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying RAG: {str(e)}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5726e1",
   "metadata": {},
   "source": [
    "### Define and execute MCP server (HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from fastmcp import FastMCP\n",
    "from uuid import UUID\n",
    "\n",
    "mcp_http_server = FastMCP(\"MCP-HTTP-Server\")\n",
    "\n",
    "# Initialize diagnostic knowledge base\n",
    "diagnostic_kb = DiagnosticKB()\n",
    "\n",
    "# -------------------------------\n",
    "# Return types\n",
    "# -------------------------------\n",
    "\n",
    "@dataclass\n",
    "class RequestLatencies:\n",
    "    storage_latency_ms: int\n",
    "    server_latency_ms: int\n",
    "    end_to_end_latency_ms: int\n",
    "\n",
    "@dataclass\n",
    "class ServiceInfo:\n",
    "    service_type: str\n",
    "    account_id: UUID\n",
    "    deployment_id: UUID\n",
    "\n",
    "@dataclass\n",
    "class DeploymentInfo:\n",
    "    vm_type: str\n",
    "    role_instances_count: int\n",
    "    subscription_id: UUID\n",
    "\n",
    "@dataclass\n",
    "class ErrorInfo:\n",
    "    type: str\n",
    "    message: str\n",
    "\n",
    "@dataclass\n",
    "class ErrorResponse:\n",
    "    error: ErrorInfo\n",
    "\n",
    "# -------------------------------\n",
    "# Tools\n",
    "# -------------------------------\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def failure_analysis_kb(query: str):\n",
    "    \"\"\"Knowledge base for procedures on how to diagnose issues.\"\"\"\n",
    "    return diagnostic_kb.query_rag(query)\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_system_load(query: str) -> dict:\n",
    "    \"\"\"Provides current system load\"\"\"\n",
    "\n",
    "    return { \"sytem_load_percent\": 99 }\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_system_latency(query: str) -> dict:\n",
    "    \"\"\"Provides current system latency\"\"\"\n",
    "\n",
    "    return { \"system_latency_ms\": 100 }\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_link_status(query: str) -> dict:\n",
    "    \"\"\"Reports whether the link status is connected or disconnected\"\"\"\n",
    "\n",
    "    response = { \"link_status\": \"disconnected\" }\n",
    "    if random.randint(1, 100) < 50:\n",
    "        response = { \"link_status\": \"connected\" }\n",
    "    return response\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_high_latency_request_percentage(service_id: str, time_window: int) -> dict | ErrorResponse:\n",
    "    \"\"\"Reports the percentage of high latency requests on {service-id} in the last {time_window} hours\"\"\"\n",
    "\n",
    "    if service_id == \"d3f1a8b2-7c4e-4f9e-9e2a-8b6c3a2d1f4e\":\n",
    "        return { \"high_latency_requests_percent\": 98 }\n",
    "\n",
    "    return ErrorResponse(ErrorInfo(\n",
    "        type=\"invalid parameter\",\n",
    "        message=\"service_id not found\"))\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_average_request_latencies(service_id: str, time_window: int) -> RequestLatencies | ErrorResponse:\n",
    "    \"\"\"Reports average request latencies on {service-id} in the last {time_window} hours \"\"\"\n",
    "\n",
    "    if service_id == \"d3f1a8b2-7c4e-4f9e-9e2a-8b6c3a2d1f4e\":\n",
    "        return RequestLatencies(\n",
    "            storage_latency_ms=10,\n",
    "            server_latency_ms=500,\n",
    "            end_to_end_latency_ms=2000)\n",
    "\n",
    "    return ErrorResponse(ErrorInfo(\n",
    "        type=\"invalid parameter\",\n",
    "        message=\"service_id not found\"))\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_service_info(service_id: str) -> ServiceInfo | ErrorResponse:\n",
    "    \"\"\"Reports information on {service-id} \"\"\"\n",
    "\n",
    "    if service_id == \"d3f1a8b2-7c4e-4f9e-9e2a-8b6c3a2d1f4e\":\n",
    "        return ServiceInfo(\n",
    "            service_type=\"shared origin\",\n",
    "            account_id=uuid.UUID(\"a1d4c6f7-3e2b-4b9a-bc8f-9f6e2a1d7c3e\"),\n",
    "            deployment_id=uuid.UUID(\"f3c9a7e2-8b4d-4f6a-9c2e-7d1b3a6e5c9f\"))\n",
    "\n",
    "    return ErrorResponse(ErrorInfo(\n",
    "        type=\"invalid parameter\",\n",
    "        message=\"service_id not found\"))\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_deployment_info(deployment_id: str) -> DeploymentInfo | ErrorResponse:\n",
    "    \"\"\"Reports information on {deployment_id}\"\"\"\n",
    "\n",
    "    if deployment_id == \"f3c9a7e2-8b4d-4f6a-9c2e-7d1b3a6e5c9f\":\n",
    "        return DeploymentInfo(\n",
    "            vm_type=\"Standard_D16_v5\",\n",
    "            role_instances_count=2,\n",
    "            subscription_id=uuid.UUID(\"c7e2b9f1-4d3a-4a8e-9f6c-2b1d7e3f9a4c\"))\n",
    "\n",
    "    return ErrorResponse(ErrorInfo(\n",
    "        type=\"invalid parameter\",\n",
    "        message=\"deployment_id not found\"))\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_average_cpu_load(deployment_id: str, time_window: int) -> dict | ErrorResponse:\n",
    "    \"\"\"Reports the average CPU load on {deployment-id} in the last {time_window} hours\"\"\"\n",
    "\n",
    "    if deployment_id == \"f3c9a7e2-8b4d-4f6a-9c2e-7d1b3a6e5c9f\":\n",
    "        return { \"average_cpu_load_percent\": 98 }\n",
    "\n",
    "    return ErrorResponse(ErrorInfo(\n",
    "        type=\"invalid parameter\",\n",
    "        message=\"deployment_id not found\"))\n",
    "\n",
    "@mcp_http_server.tool()\n",
    "def query_average_requests_per_sec(deployment_id: str, time_window: int) -> dict | ErrorResponse:\n",
    "    \"\"\"Reports the average requests per second on {deployment-id} in the last {time_window} hours\"\"\"\n",
    "\n",
    "    if deployment_id == \"f3c9a7e2-8b4d-4f6a-9c2e-7d1b3a6e5c9f\":\n",
    "        return { \"average_requests_per_second\": 500 }\n",
    "\n",
    "    return ErrorResponse(ErrorInfo(\n",
    "        type=\"invalid parameter\",\n",
    "        message=\"deployment_id not found\"))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt\n",
    "# -------------------------------\n",
    "\n",
    "@mcp_http_server.prompt()\n",
    "def get_llm_prompt(query: str) -> str:\n",
    "    \"\"\"Generates a prompt for the LLM to use to answer the query\"\"\"\n",
    "\n",
    "    raise Exception(\"Not implemented\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    from threading import Thread\n",
    "\n",
    "    def start():\n",
    "        mcp_http_server.run(transport=\"streamable-http\",\n",
    "                            host=\"localhost\",\n",
    "                            port=8000,\n",
    "                            path=\"/mcp\",\n",
    "                            log_level=\"debug\")\n",
    "\n",
    "    Thread(target=start, daemon=True).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b2132",
   "metadata": {},
   "source": [
    "### Check whether MCP server is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767450bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lsof -i :8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!netstat -a | grep 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!netstat -an | findstr 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ecee8",
   "metadata": {},
   "source": [
    "### Stop the MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lsof -t -i:8000 | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1484c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tasklist | findstr python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!taskkill /PID <PID> /F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb801df4",
   "metadata": {},
   "source": [
    "### Define and Run Diagnostic Agent / MCP Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostic_agent.py\n",
    "#\n",
    "# Move tools from STDIO to HTTP server\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "def get_llm(name: str|None):\n",
    "    name = name or \"gemini-2.5-flash\"\n",
    "    return ChatGoogleGenerativeAI(model=name)\n",
    "\n",
    "def get_llm_prompt(query: str) -> str:\n",
    "    return f\"\"\"\n",
    "    You are a helpful assistant. Answer the following query\n",
    "    by only using the tools provided to you. DO NOT make up any information.\n",
    "    Query failure analysis KB first to make a plan on how to derive the\n",
    "    response. Then execute the plan until root cause is determined for response.\n",
    "    Do not repeat tool calls with the same query.\n",
    "\n",
    "    If the result of a step in the plan indicates a new KB query is needed\n",
    "    for root causing the issue then you MUST perform the new failure analysis KB query\n",
    "    and make a new plan to derive the root cause response. DO NOT finish before\n",
    "    the root cause is determined, unless the conclusion is \"unable to determine\n",
    "    root cause\".\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "async def query_agent(prompt: str, model: str|None=None) -> str:\n",
    "\n",
    "    # MCP server accessible through HTTP\n",
    "    mcp_http_server_url=\"http://localhost:8000/mcp\"\n",
    "\n",
    "    try:\n",
    "        async with streamablehttp_client(mcp_http_server_url) as (http_read, http_write, _):\n",
    "            async with ClientSession(http_read, http_write) as http_session:\n",
    "                print(\"initializing HTTP client session\")\n",
    "                await http_session.initialize()\n",
    "\n",
    "                print(\"\\nloading tools & prompt\")\n",
    "                # Tool names must be unique, the LLM will chose\n",
    "                # a tool based on the query and tool descriptions.\n",
    "                mcp_server_tools = await load_mcp_tools(http_session)\n",
    "\n",
    "                print(\"\\nTools loaded :\")\n",
    "                for tool in mcp_server_tools:\n",
    "                    print(f\"▪️ {tool.name} - {tool.description}\")\n",
    "\n",
    "                llm = get_llm(model)\n",
    "                llm_prompt = get_llm_prompt(prompt)\n",
    "\n",
    "                config = RunnableConfig()\n",
    "\n",
    "                agent=create_react_agent(model=llm, tools=mcp_server_tools, debug=False)\n",
    "\n",
    "                print(f\"\\nAnswering query : {prompt}\")\n",
    "                agent_response = await agent.ainvoke(input={\"messages\": llm_prompt}, config=config)\n",
    "\n",
    "                return agent_response[\"messages\"][-1].content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if isinstance(e, ExceptionGroup):\n",
    "            print(f\"{e.exceptions}\")\n",
    "        return \"Error\"\n",
    "\n",
    "    return \"Error\"\n",
    "\n",
    "\n",
    "# main\n",
    "print(\"\\nRunning Query Agent...\")\n",
    "response = await query_agent(\n",
    "        prompt=\"What is the cause of alert 'Origin service d3f1a8b2-7c4e-4f9e-9e2a-8b6c3a2d1f4e with high latency on more than 90% of requests in the last hour'\",\n",
    "        model=None)\n",
    "\n",
    "print(\"\\nResponse: \", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
